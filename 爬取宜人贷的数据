# spyder
# -*- coding: utf-8 -*-
"""
Created on Mon Aug  7 11:06:23 2017

@author: guiyanyan
"""
import re
import requests
from bs4 import BeautifulSoup as bs
import pandas as pd
import datetime
import time
import string
from lxml import etree

starttime = datetime.datetime.now()


head = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:51.0) Gecko/20100101 Firefox/51.0'}

base_url = 'http://tieba.baidu.com/f?kw=%E5%AE%9C%E4%BA%BA%E8%B4%B7&ie=utf-8'
all_page =50
url_list = []

# 将所有需要爬去的url存入列表
for i in range(1, all_page):
    url_list.append(base_url + '&pn=' + str(50 * i))
    print('第',i+1,'页url已经拼接完成!')


res=[];res2=[];res3=[];i1=0;res31=[];
for url in url_list:
    i1=i1+1
    res.append(requests.get(url,headers=head))
    time.sleep(0.1)
    print('开始对第',i1,'页发起请求')


for html in res:
    res2.append(html.text.replace('<!--',''))

for html in res2:
    res31.append(html.replace('-->',''))

for html in res31:
    res3.append(html.replace('<cc>',''))

print('替换完所有的注释符号！')

soup=[]
for i in res3:
    soup.append(bs(i,'lxml'))

title = [] #标题
author = [] # 作者
time_send=[] #发帖时间
reply_num=[]#帖子回复数
tiezi_url =[]#帖子ID
first_comment=['目前手机贷客服业务量大增，导致许多用户无法拨通手机贷客服热线']   #第一楼
i3=119
for s in soup:
    i3=i3+1

    a =s.find_all('a',class_='j_th_tit ')
    for i in a:
        title.append(i.string)

    b = s.find_all('span', title=re.compile('(主题作者.*)'))
    for i in b:
        author.append(i.get_text())

    c = s.find_all('span',class_='pull-right is_show_create_time')
    for i in c:
        time_send.append(i.get_text())

    d= s.find_all('span',class_='threadlist_rep_num center_text')
    for i in d:
        reply_num.append(i.get_text())

    e = s.find_all('a', class_='j_th_tit ')

    for i in e:
        tiezi_url.append('https://tieba.baidu.com'+i.get('href'))
    #第一楼
    f=s.find_all('div', class_='threadlist_abs threadlist_abs_onlyline ')
    for i in f:
        first_comment.append(i.get_text())
    print('第', i3, '页帖子数据爬取完毕')

# 导出数据
print('导出帖子标题数据！')
list_1 = [title,author,time_send,reply_num,first_comment]
data1 = pd.DataFrame(list_1)
data1.T.to_csv('yirendai_title_1_50.csv')

print('开始爬取每个帖子的回复内容...')
print('正在爬取第一个帖子的回复内容...')
# 爬取每个帖子里的回复
#===========================================================
all_comment=[[]] #记录一个帖子内的所有回复
all_user_name=[]
all_comment_time=[]
all_title=[]
def get_page_num(url):
    res = requests.get(url,headers=head).text
    soup = bs(res,'lxml')
    if soup.find('title').get_text()=='贴吧404' or soup.find('title').get_text()=='404 Not Found' :
        page_num = 0
        return page_num
    else:
        try:
            s = soup.find_all('li', class_='l_reply_num')[2].get_text()
        except:
            page_num = 0
            return page_num
        else:
            a = re.findall('共(.*)页', s)
            page_num = int(a[0])
            return page_num

def openlink(url):
    maxTryNum = 10
    for tries in range(maxTryNum):
        try:
            res =requests.get(url, headers=head).text
            return res
        except:
            if tries < (maxTryNum - 1):
                continue
            else:
                print("Has tried %d times to access url %s, all failed!", maxTryNum, url)
                break



def get_all_comment(url):
    url_list = [];
    res = [];soup = [];comment = [];user_name=[];comment_time=[];soup_1=[];soup_2=[];
    for i in range(1, get_page_num(url)+1):
        if get_page_num(url) ==0:
            continue
        url_list.append(url + '?&pn=' + str(i))
    for i in url_list:
        res.append(openlink(i))
        time.sleep(0.5)
    for i in res:
        soup.append(bs(i,'lxml'))   
    
    for i  in res:
        soup_1.append(etree.HTML(i))
        
    
    for s in soup:
        user_name.append(s.find_all('img', username=re.compile('\S')))   
        stem=str(s)
        comment_time.append(re.findall('\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}',stem))     
        # 只保留时间，用正则表达式匹配开始前四位都是数字的
    
    for i in soup_1:
        soup_2.append(i.xpath('//cc//div[@class]')) 
       
    for ss in range(len(soup_2)):
        for sj in range(len(soup_2[ss])):
            comment.append(soup_2[ss][sj].text)
           
    for i in range(len(comment)):
            all_comment.append(comment[i])
            all_title.append(title[i5-1])
    for i in user_name:
        for j in range(len(i)):
            all_user_name.append(i[j].get('username'))
    for i in comment_time:
        for j in range(len(i)):
            all_comment_time.append(i[j])

i5=0
for url in tiezi_url:
    
    i5=1+i5
    get_all_comment(url)
    print('共有',50*50,'个帖子','已经爬取了第',i5,'个帖子')

print(len(all_comment))
print(len(all_comment_time))
print(len(all_user_name))

# 导出数据
list_2 = [all_title,all_comment,all_comment_time,all_user_name]

data2 = pd.DataFrame(list_2)



print('开始导出回复数据！')
data2.T.to_csv('yirendai_comment_1_50.csv')

endtime = datetime.datetime.now()
interval = (endtime - starttime).seconds
print('共消耗时间：',interval,'秒')
print('Done!')
